{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['TF_NUM_INTRAOP_THREADS'] = '1'\n",
    "os.environ['TF_NUM_INTEROP_THREADS'] = '1'\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 28\n",
    "img_flat_size = img_size * img_size\n",
    "\n",
    "num_label = 10\n",
    "\n",
    "# Parameters of training\n",
    "Learning_rate = 0.0005\n",
    "epsilon = 1e-2\n",
    "\n",
    "num_iter = 5100\n",
    "batch_size = 128\n",
    "\n",
    "validation_ratio = 0.1\n",
    "\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (48000, 784)\n",
      "Y_train shape: (48000, 10)\n",
      "X_val shape: (12000, 784)\n",
      "Y_val shape: (12000, 10)\n",
      "X_test shape: (10000, 784)\n",
      "Y_test shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "(X, Y), (X_test, Y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X = X.astype(\"float32\") / 255.0\n",
    "X_test = X_test.astype(\"float32\") / 255.0\n",
    "Y = to_categorical(Y, num_classes=num_label)\n",
    "Y_test = to_categorical(Y_test, num_classes=num_label)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "X_train = X_train.reshape(-1, img_flat_size)\n",
    "X_val = X_val.reshape(-1, img_flat_size)\n",
    "X_test = X_test.reshape(-1, img_flat_size)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"Y_val shape:\", Y_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Y_test shape:\", Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NotMNIST X shape: (18724, 784)\n",
      "NotMNIST Y shape: (18724, 10)\n"
     ]
    }
   ],
   "source": [
    "folders_notMNIST = os.listdir('./notMNIST_small')\n",
    "\n",
    "NotMNIST_x_list = []\n",
    "NotMNIST_y_list = []\n",
    "\n",
    "for idx, folder in enumerate(folders_notMNIST):\n",
    "    files_notMNIST = os.listdir('./notMNIST_small/' + folder)\n",
    "    \n",
    "    for file in files_notMNIST:\n",
    "        img_NotMNIST = cv2.imread('./notMNIST_small/' + folder + '/' + file, 0)\n",
    "        if img_NotMNIST is None:\n",
    "            continue\n",
    "        NotMNIST_flat = np.reshape(img_NotMNIST, (img_flat_size))\n",
    "        NotMNIST_x_list.append(NotMNIST_flat)\n",
    "        \n",
    "        label_temp = np.zeros([num_label])\n",
    "        label_temp[idx] = 1\n",
    "        \n",
    "        NotMNIST_y_list.append(label_temp)\n",
    "        \n",
    "NotMNIST_x = np.stack(NotMNIST_x_list, axis = 0)\n",
    "NotMNIST_y = np.stack(NotMNIST_y_list, axis = 0)\n",
    "\n",
    "print(\"NotMNIST X shape: \" + str(NotMNIST_x.shape))\n",
    "print(\"NotMNIST Y shape: \" + str(NotMNIST_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1/5\n",
      "Epoch 1/10\n",
      "375/375 - 3s - 8ms/step - accuracy: 0.9136 - loss: 0.2983 - val_accuracy: 0.9588 - val_loss: 0.1353\n",
      "Epoch 2/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9680 - loss: 0.1084 - val_accuracy: 0.9679 - val_loss: 0.1024\n",
      "Epoch 3/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9779 - loss: 0.0719 - val_accuracy: 0.9696 - val_loss: 0.1017\n",
      "Epoch 4/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9844 - loss: 0.0507 - val_accuracy: 0.9692 - val_loss: 0.1068\n",
      "Epoch 5/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9877 - loss: 0.0386 - val_accuracy: 0.9730 - val_loss: 0.1007\n",
      "Epoch 6/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9890 - loss: 0.0326 - val_accuracy: 0.9728 - val_loss: 0.1084\n",
      "Epoch 7/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9918 - loss: 0.0254 - val_accuracy: 0.9695 - val_loss: 0.1293\n",
      "Epoch 8/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9921 - loss: 0.0239 - val_accuracy: 0.9750 - val_loss: 0.1091\n",
      "Epoch 9/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9937 - loss: 0.0190 - val_accuracy: 0.9731 - val_loss: 0.1316\n",
      "Epoch 10/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9925 - loss: 0.0212 - val_accuracy: 0.9726 - val_loss: 0.1244\n",
      "Training model 2/5\n",
      "Epoch 1/10\n",
      "375/375 - 3s - 8ms/step - accuracy: 0.9126 - loss: 0.2955 - val_accuracy: 0.9574 - val_loss: 0.1414\n",
      "Epoch 2/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9665 - loss: 0.1109 - val_accuracy: 0.9591 - val_loss: 0.1276\n",
      "Epoch 3/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9784 - loss: 0.0716 - val_accuracy: 0.9693 - val_loss: 0.0984\n",
      "Epoch 4/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9848 - loss: 0.0499 - val_accuracy: 0.9714 - val_loss: 0.1020\n",
      "Epoch 5/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9878 - loss: 0.0388 - val_accuracy: 0.9735 - val_loss: 0.0976\n",
      "Epoch 6/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9899 - loss: 0.0319 - val_accuracy: 0.9758 - val_loss: 0.0902\n",
      "Epoch 7/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9919 - loss: 0.0251 - val_accuracy: 0.9743 - val_loss: 0.0996\n",
      "Epoch 8/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9918 - loss: 0.0242 - val_accuracy: 0.9732 - val_loss: 0.1064\n",
      "Epoch 9/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9929 - loss: 0.0214 - val_accuracy: 0.9760 - val_loss: 0.1096\n",
      "Epoch 10/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9941 - loss: 0.0184 - val_accuracy: 0.9778 - val_loss: 0.1025\n",
      "Training model 3/5\n",
      "Epoch 1/10\n",
      "375/375 - 3s - 8ms/step - accuracy: 0.9136 - loss: 0.2954 - val_accuracy: 0.9575 - val_loss: 0.1399\n",
      "Epoch 2/10\n",
      "375/375 - 2s - 6ms/step - accuracy: 0.9663 - loss: 0.1115 - val_accuracy: 0.9648 - val_loss: 0.1132\n",
      "Epoch 3/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9762 - loss: 0.0748 - val_accuracy: 0.9702 - val_loss: 0.1002\n",
      "Epoch 4/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9838 - loss: 0.0522 - val_accuracy: 0.9700 - val_loss: 0.1077\n",
      "Epoch 5/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9883 - loss: 0.0387 - val_accuracy: 0.9697 - val_loss: 0.1153\n",
      "Epoch 6/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9896 - loss: 0.0335 - val_accuracy: 0.9722 - val_loss: 0.1061\n",
      "Epoch 7/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9904 - loss: 0.0296 - val_accuracy: 0.9749 - val_loss: 0.1006\n",
      "Epoch 8/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9910 - loss: 0.0265 - val_accuracy: 0.9758 - val_loss: 0.1037\n",
      "Epoch 9/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9942 - loss: 0.0184 - val_accuracy: 0.9750 - val_loss: 0.1101\n",
      "Epoch 10/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9938 - loss: 0.0185 - val_accuracy: 0.9716 - val_loss: 0.1385\n",
      "Training model 4/5\n",
      "Epoch 1/10\n",
      "375/375 - 3s - 8ms/step - accuracy: 0.9151 - loss: 0.2984 - val_accuracy: 0.9568 - val_loss: 0.1398\n",
      "Epoch 2/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9676 - loss: 0.1083 - val_accuracy: 0.9620 - val_loss: 0.1200\n",
      "Epoch 3/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9779 - loss: 0.0721 - val_accuracy: 0.9672 - val_loss: 0.1049\n",
      "Epoch 4/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9846 - loss: 0.0499 - val_accuracy: 0.9677 - val_loss: 0.1087\n",
      "Epoch 5/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9866 - loss: 0.0408 - val_accuracy: 0.9655 - val_loss: 0.1216\n",
      "Epoch 6/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9890 - loss: 0.0345 - val_accuracy: 0.9723 - val_loss: 0.1117\n",
      "Epoch 7/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9901 - loss: 0.0290 - val_accuracy: 0.9744 - val_loss: 0.1060\n",
      "Epoch 8/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9919 - loss: 0.0247 - val_accuracy: 0.9722 - val_loss: 0.1134\n",
      "Epoch 9/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9933 - loss: 0.0205 - val_accuracy: 0.9748 - val_loss: 0.1141\n",
      "Epoch 10/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9938 - loss: 0.0189 - val_accuracy: 0.9786 - val_loss: 0.0957\n",
      "Training model 5/5\n",
      "Epoch 1/10\n",
      "375/375 - 3s - 9ms/step - accuracy: 0.9145 - loss: 0.2971 - val_accuracy: 0.9602 - val_loss: 0.1343\n",
      "Epoch 2/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9680 - loss: 0.1071 - val_accuracy: 0.9670 - val_loss: 0.1074\n",
      "Epoch 3/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9783 - loss: 0.0704 - val_accuracy: 0.9649 - val_loss: 0.1145\n",
      "Epoch 4/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9850 - loss: 0.0495 - val_accuracy: 0.9677 - val_loss: 0.1063\n",
      "Epoch 5/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9887 - loss: 0.0359 - val_accuracy: 0.9740 - val_loss: 0.0969\n",
      "Epoch 6/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9899 - loss: 0.0318 - val_accuracy: 0.9688 - val_loss: 0.1216\n",
      "Epoch 7/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9899 - loss: 0.0293 - val_accuracy: 0.9710 - val_loss: 0.1148\n",
      "Epoch 8/10\n",
      "375/375 - 2s - 6ms/step - accuracy: 0.9908 - loss: 0.0275 - val_accuracy: 0.9736 - val_loss: 0.1087\n",
      "Epoch 9/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9938 - loss: 0.0188 - val_accuracy: 0.9747 - val_loss: 0.1084\n",
      "Epoch 10/10\n",
      "375/375 - 2s - 6ms/step - accuracy: 0.9940 - loss: 0.0179 - val_accuracy: 0.9760 - val_loss: 0.1118\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "Ensemble accuracy on test set: 0.9814\n",
      "Variance for the first test sample: [1.4035959e-19 2.8614763e-16 7.1555888e-16 4.7170901e-15 2.7313581e-19\n",
      " 8.6805667e-23 3.9426263e-26 2.8946728e-10 5.8192580e-19 2.9177732e-10]\n"
     ]
    }
   ],
   "source": [
    "def create_ensemble_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(img_flat_size,)),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(num_label, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "ensemble_size = 5\n",
    "models_ensemble = []\n",
    "\n",
    "for i in range(ensemble_size):\n",
    "    print(f\"Training model {i + 1}/{ensemble_size}\")\n",
    "    model = create_ensemble_model()\n",
    "    model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=10, batch_size=128, verbose=2)\n",
    "    models_ensemble.append(model)\n",
    "\n",
    "def ensemble_predictions(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models])\n",
    "    mean_prediction = predictions.mean(axis=0)\n",
    "    variance_prediction = predictions.var(axis=0)\n",
    "    return mean_prediction, variance_prediction\n",
    "\n",
    "mean_pred, var_pred = ensemble_predictions(models_ensemble, X_test)\n",
    "\n",
    "predicted_classes = np.argmax(mean_pred, axis=1)\n",
    "true_classes = np.argmax(Y_test, axis=1)\n",
    "accuracy = np.mean(predicted_classes == true_classes)\n",
    "print(f\"Ensemble accuracy on test set: {accuracy:.4f}\")\n",
    "print(f\"Variance for the first test sample: {var_pred[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 - 3s - 8ms/step - accuracy: 0.8769 - loss: 0.4000 - val_accuracy: 0.9562 - val_loss: 0.1447\n",
      "Epoch 2/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9509 - loss: 0.1634 - val_accuracy: 0.9680 - val_loss: 0.1083\n",
      "Epoch 3/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9635 - loss: 0.1200 - val_accuracy: 0.9700 - val_loss: 0.0960\n",
      "Epoch 4/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9698 - loss: 0.0963 - val_accuracy: 0.9722 - val_loss: 0.0939\n",
      "Epoch 5/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9747 - loss: 0.0818 - val_accuracy: 0.9741 - val_loss: 0.0870\n",
      "Epoch 6/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9780 - loss: 0.0696 - val_accuracy: 0.9762 - val_loss: 0.0842\n",
      "Epoch 7/10\n",
      "375/375 - 2s - 4ms/step - accuracy: 0.9801 - loss: 0.0614 - val_accuracy: 0.9759 - val_loss: 0.0849\n",
      "Epoch 8/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9819 - loss: 0.0558 - val_accuracy: 0.9787 - val_loss: 0.0797\n",
      "Epoch 9/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9841 - loss: 0.0524 - val_accuracy: 0.9774 - val_loss: 0.0839\n",
      "Epoch 10/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9840 - loss: 0.0480 - val_accuracy: 0.9787 - val_loss: 0.0822\n",
      "Ensemble accuracy with MC Dropout on test set: 0.9771\n",
      "Variance for the first test sample with MC Dropout: [2.31353649e-14 2.58845528e-10 1.10949635e-11 2.42242088e-10\n",
      " 5.67575240e-12 1.10186773e-11 2.31454473e-20 4.84961626e-09\n",
      " 4.15407465e-12 3.05936831e-09]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_MC_Dropout_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(28 * 28,)),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dropout(0.2),  \n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dropout(0.2), \n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_mc_dropout = create_MC_Dropout_model()\n",
    "model_mc_dropout.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=10, batch_size=128, verbose=2)\n",
    "\n",
    "def mc_dropout_inference(model, X, n_iter=5):\n",
    "    @tf.function\n",
    "    def predict_with_dropout(X):\n",
    "        return model(X, training=True)  # Ensure dropout is active during inference\n",
    "\n",
    "    predictions = np.array([predict_with_dropout(X).numpy() for _ in range(n_iter)])\n",
    "    \n",
    "    mean_prediction = predictions.mean(axis=0)\n",
    "    variance_prediction = predictions.var(axis=0)\n",
    "    \n",
    "    return mean_prediction, variance_prediction\n",
    "\n",
    "mean_pred_mc, var_pred_mc = mc_dropout_inference(model_mc_dropout, X_test, n_iter=5)\n",
    "\n",
    "predicted_classes = np.argmax(mean_pred_mc, axis=1)\n",
    "true_classes = np.argmax(Y_test, axis=1)\n",
    "accuracy = np.mean(predicted_classes == true_classes)\n",
    "print(f\"Ensemble accuracy with MC Dropout on test set: {accuracy:.4f}\")\n",
    "print(f\"Variance for the first test sample with MC Dropout: {var_pred_mc[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1/5\n",
      "Epoch 1/10\n",
      "375/375 - 3s - 9ms/step - accuracy: 0.9167 - loss: 0.2889 - val_accuracy: 0.9543 - val_loss: 0.1454\n",
      "Epoch 2/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9722 - loss: 0.0926 - val_accuracy: 0.9588 - val_loss: 0.1295\n",
      "Epoch 3/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9842 - loss: 0.0531 - val_accuracy: 0.9676 - val_loss: 0.1124\n",
      "Epoch 4/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9893 - loss: 0.0339 - val_accuracy: 0.9702 - val_loss: 0.1099\n",
      "Epoch 5/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9928 - loss: 0.0232 - val_accuracy: 0.9709 - val_loss: 0.1142\n",
      "Epoch 6/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9941 - loss: 0.0184 - val_accuracy: 0.9628 - val_loss: 0.1549\n",
      "Epoch 7/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9946 - loss: 0.0159 - val_accuracy: 0.9694 - val_loss: 0.1343\n",
      "Epoch 8/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9954 - loss: 0.0131 - val_accuracy: 0.9717 - val_loss: 0.1460\n",
      "Epoch 9/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9948 - loss: 0.0149 - val_accuracy: 0.9740 - val_loss: 0.1372\n",
      "Epoch 10/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9951 - loss: 0.0142 - val_accuracy: 0.9737 - val_loss: 0.1357\n",
      "Training model 2/5\n",
      "Epoch 1/10\n",
      "375/375 - 3s - 8ms/step - accuracy: 0.9155 - loss: 0.2918 - val_accuracy: 0.9556 - val_loss: 0.1492\n",
      "Epoch 2/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9716 - loss: 0.0947 - val_accuracy: 0.9619 - val_loss: 0.1194\n",
      "Epoch 3/10\n",
      "375/375 - 2s - 6ms/step - accuracy: 0.9842 - loss: 0.0538 - val_accuracy: 0.9690 - val_loss: 0.1036\n",
      "Epoch 4/10\n",
      "375/375 - 2s - 6ms/step - accuracy: 0.9902 - loss: 0.0335 - val_accuracy: 0.9699 - val_loss: 0.1145\n",
      "Epoch 5/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9915 - loss: 0.0279 - val_accuracy: 0.9678 - val_loss: 0.1331\n",
      "Epoch 6/10\n",
      "375/375 - 2s - 6ms/step - accuracy: 0.9916 - loss: 0.0248 - val_accuracy: 0.9717 - val_loss: 0.1191\n",
      "Epoch 7/10\n",
      "375/375 - 2s - 6ms/step - accuracy: 0.9928 - loss: 0.0212 - val_accuracy: 0.9677 - val_loss: 0.1367\n",
      "Epoch 8/10\n",
      "375/375 - 2s - 6ms/step - accuracy: 0.9954 - loss: 0.0137 - val_accuracy: 0.9712 - val_loss: 0.1398\n",
      "Epoch 9/10\n",
      "375/375 - 2s - 6ms/step - accuracy: 0.9965 - loss: 0.0103 - val_accuracy: 0.9756 - val_loss: 0.1217\n",
      "Epoch 10/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9955 - loss: 0.0133 - val_accuracy: 0.9762 - val_loss: 0.1156\n",
      "Training model 3/5\n",
      "Epoch 1/10\n",
      "375/375 - 3s - 9ms/step - accuracy: 0.9171 - loss: 0.2878 - val_accuracy: 0.9597 - val_loss: 0.1354\n",
      "Epoch 2/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9746 - loss: 0.0886 - val_accuracy: 0.9672 - val_loss: 0.1102\n",
      "Epoch 3/10\n",
      "375/375 - 2s - 6ms/step - accuracy: 0.9847 - loss: 0.0529 - val_accuracy: 0.9711 - val_loss: 0.0986\n",
      "Epoch 4/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9893 - loss: 0.0362 - val_accuracy: 0.9690 - val_loss: 0.1121\n",
      "Epoch 5/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9922 - loss: 0.0259 - val_accuracy: 0.9690 - val_loss: 0.1185\n",
      "Epoch 6/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9936 - loss: 0.0199 - val_accuracy: 0.9743 - val_loss: 0.1071\n",
      "Epoch 7/10\n",
      "375/375 - 2s - 6ms/step - accuracy: 0.9935 - loss: 0.0190 - val_accuracy: 0.9721 - val_loss: 0.1168\n",
      "Epoch 8/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9951 - loss: 0.0147 - val_accuracy: 0.9741 - val_loss: 0.1155\n",
      "Epoch 9/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9972 - loss: 0.0094 - val_accuracy: 0.9736 - val_loss: 0.1249\n",
      "Epoch 10/10\n",
      "375/375 - 2s - 6ms/step - accuracy: 0.9948 - loss: 0.0161 - val_accuracy: 0.9702 - val_loss: 0.1391\n",
      "Training model 4/5\n",
      "Epoch 1/10\n",
      "375/375 - 3s - 9ms/step - accuracy: 0.9170 - loss: 0.2867 - val_accuracy: 0.9535 - val_loss: 0.1529\n",
      "Epoch 2/10\n",
      "375/375 - 2s - 6ms/step - accuracy: 0.9726 - loss: 0.0915 - val_accuracy: 0.9624 - val_loss: 0.1239\n",
      "Epoch 3/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9841 - loss: 0.0526 - val_accuracy: 0.9628 - val_loss: 0.1310\n",
      "Epoch 4/10\n",
      "375/375 - 2s - 6ms/step - accuracy: 0.9887 - loss: 0.0351 - val_accuracy: 0.9709 - val_loss: 0.1121\n",
      "Epoch 5/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9910 - loss: 0.0280 - val_accuracy: 0.9658 - val_loss: 0.1547\n",
      "Epoch 6/10\n",
      "375/375 - 2s - 6ms/step - accuracy: 0.9934 - loss: 0.0207 - val_accuracy: 0.9697 - val_loss: 0.1273\n",
      "Epoch 7/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9942 - loss: 0.0180 - val_accuracy: 0.9707 - val_loss: 0.1222\n",
      "Epoch 8/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9958 - loss: 0.0134 - val_accuracy: 0.9730 - val_loss: 0.1257\n",
      "Epoch 9/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9951 - loss: 0.0141 - val_accuracy: 0.9698 - val_loss: 0.1572\n",
      "Epoch 10/10\n",
      "375/375 - 2s - 6ms/step - accuracy: 0.9961 - loss: 0.0115 - val_accuracy: 0.9693 - val_loss: 0.1527\n",
      "Training model 5/5\n",
      "Epoch 1/10\n",
      "375/375 - 3s - 9ms/step - accuracy: 0.9159 - loss: 0.2904 - val_accuracy: 0.9532 - val_loss: 0.1549\n",
      "Epoch 2/10\n",
      "375/375 - 2s - 6ms/step - accuracy: 0.9725 - loss: 0.0908 - val_accuracy: 0.9659 - val_loss: 0.1148\n",
      "Epoch 3/10\n",
      "375/375 - 2s - 6ms/step - accuracy: 0.9844 - loss: 0.0518 - val_accuracy: 0.9683 - val_loss: 0.1100\n",
      "Epoch 4/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9893 - loss: 0.0348 - val_accuracy: 0.9679 - val_loss: 0.1204\n",
      "Epoch 5/10\n",
      "375/375 - 2s - 6ms/step - accuracy: 0.9913 - loss: 0.0276 - val_accuracy: 0.9628 - val_loss: 0.1567\n",
      "Epoch 6/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9933 - loss: 0.0205 - val_accuracy: 0.9714 - val_loss: 0.1265\n",
      "Epoch 7/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9941 - loss: 0.0171 - val_accuracy: 0.9701 - val_loss: 0.1274\n",
      "Epoch 8/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9955 - loss: 0.0141 - val_accuracy: 0.9701 - val_loss: 0.1416\n",
      "Epoch 9/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9952 - loss: 0.0148 - val_accuracy: 0.9737 - val_loss: 0.1270\n",
      "Epoch 10/10\n",
      "375/375 - 2s - 6ms/step - accuracy: 0.9957 - loss: 0.0125 - val_accuracy: 0.9745 - val_loss: 0.1358\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Bootstrapped ensemble accuracy on test set: 0.9822\n",
      "Variance for the first test sample: [4.5049380e-22 4.4127244e-18 1.5159119e-18 1.4537869e-14 3.4130209e-20\n",
      " 9.9816173e-22 1.9609996e-29 2.2737368e-14 2.2001011e-20 2.4071750e-16]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_bootstrap_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(28 * 28,)),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "ensemble_size = 5\n",
    "models_bootstrap = []\n",
    "\n",
    "for i in range(ensemble_size):\n",
    "    print(f\"Training model {i + 1}/{ensemble_size}\")\n",
    "    \n",
    "    indices = np.random.choice(X_train.shape[0], size=X_train.shape[0], replace=True)\n",
    "    X_train_bootstrap = X_train[indices]\n",
    "    Y_train_bootstrap = Y_train[indices]\n",
    "    \n",
    "    model = create_bootstrap_model()\n",
    "    model.fit(X_train_bootstrap, Y_train_bootstrap, validation_data=(X_val, Y_val), epochs=10, batch_size=128, verbose=2)\n",
    "    models_bootstrap.append(model)\n",
    "\n",
    "def ensemble_predictions(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models])\n",
    "    mean_prediction = predictions.mean(axis=0)\n",
    "    variance_prediction = predictions.var(axis=0)\n",
    "    return mean_prediction, variance_prediction\n",
    "\n",
    "mean_pred, var_pred = ensemble_predictions(models_bootstrap, X_test)\n",
    "\n",
    "predicted_classes = np.argmax(mean_pred, axis=1)\n",
    "true_classes = np.argmax(Y_test, axis=1)\n",
    "accuracy = np.mean(predicted_classes == true_classes)\n",
    "print(f\"Bootstrapped ensemble accuracy on test set: {accuracy:.4f}\")\n",
    "print(f\"Variance for the first test sample: {var_pred[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training branch model 1/5\n",
      "Epoch 1/10\n",
      "375/375 - 3s - 8ms/step - accuracy: 0.8816 - loss: 0.3908 - val_accuracy: 0.9578 - val_loss: 0.1432\n",
      "Epoch 2/10\n",
      "375/375 - 2s - 4ms/step - accuracy: 0.9537 - loss: 0.1577 - val_accuracy: 0.9653 - val_loss: 0.1130\n",
      "Epoch 3/10\n",
      "375/375 - 2s - 4ms/step - accuracy: 0.9654 - loss: 0.1158 - val_accuracy: 0.9705 - val_loss: 0.0986\n",
      "Epoch 4/10\n",
      "375/375 - 2s - 4ms/step - accuracy: 0.9696 - loss: 0.0988 - val_accuracy: 0.9754 - val_loss: 0.0865\n",
      "Epoch 5/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9754 - loss: 0.0788 - val_accuracy: 0.9758 - val_loss: 0.0849\n",
      "Epoch 6/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9770 - loss: 0.0704 - val_accuracy: 0.9764 - val_loss: 0.0820\n",
      "Epoch 7/10\n",
      "375/375 - 2s - 4ms/step - accuracy: 0.9803 - loss: 0.0640 - val_accuracy: 0.9750 - val_loss: 0.0926\n",
      "Epoch 8/10\n",
      "375/375 - 2s - 4ms/step - accuracy: 0.9818 - loss: 0.0553 - val_accuracy: 0.9786 - val_loss: 0.0774\n",
      "Epoch 9/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9841 - loss: 0.0487 - val_accuracy: 0.9785 - val_loss: 0.0847\n",
      "Epoch 10/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9844 - loss: 0.0484 - val_accuracy: 0.9786 - val_loss: 0.0782\n",
      "Training branch model 2/5\n",
      "Epoch 1/10\n",
      "375/375 - 3s - 7ms/step - accuracy: 0.8804 - loss: 0.3936 - val_accuracy: 0.9567 - val_loss: 0.1439\n",
      "Epoch 2/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9513 - loss: 0.1641 - val_accuracy: 0.9650 - val_loss: 0.1143\n",
      "Epoch 3/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9646 - loss: 0.1179 - val_accuracy: 0.9697 - val_loss: 0.1032\n",
      "Epoch 4/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9709 - loss: 0.0956 - val_accuracy: 0.9696 - val_loss: 0.1017\n",
      "Epoch 5/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9745 - loss: 0.0819 - val_accuracy: 0.9746 - val_loss: 0.0884\n",
      "Epoch 6/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9768 - loss: 0.0724 - val_accuracy: 0.9752 - val_loss: 0.0895\n",
      "Epoch 7/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9800 - loss: 0.0619 - val_accuracy: 0.9738 - val_loss: 0.0898\n",
      "Epoch 8/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9821 - loss: 0.0552 - val_accuracy: 0.9777 - val_loss: 0.0800\n",
      "Epoch 9/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9835 - loss: 0.0513 - val_accuracy: 0.9762 - val_loss: 0.0932\n",
      "Epoch 10/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9845 - loss: 0.0491 - val_accuracy: 0.9779 - val_loss: 0.0812\n",
      "Training branch model 3/5\n",
      "Epoch 1/10\n",
      "375/375 - 3s - 8ms/step - accuracy: 0.8818 - loss: 0.3919 - val_accuracy: 0.9547 - val_loss: 0.1518\n",
      "Epoch 2/10\n",
      "375/375 - 2s - 6ms/step - accuracy: 0.9519 - loss: 0.1604 - val_accuracy: 0.9665 - val_loss: 0.1088\n",
      "Epoch 3/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9635 - loss: 0.1231 - val_accuracy: 0.9708 - val_loss: 0.0953\n",
      "Epoch 4/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9699 - loss: 0.0976 - val_accuracy: 0.9740 - val_loss: 0.0871\n",
      "Epoch 5/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9745 - loss: 0.0825 - val_accuracy: 0.9758 - val_loss: 0.0849\n",
      "Epoch 6/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9783 - loss: 0.0704 - val_accuracy: 0.9778 - val_loss: 0.0817\n",
      "Epoch 7/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9799 - loss: 0.0621 - val_accuracy: 0.9759 - val_loss: 0.0898\n",
      "Epoch 8/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9815 - loss: 0.0582 - val_accuracy: 0.9777 - val_loss: 0.0895\n",
      "Epoch 9/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9836 - loss: 0.0507 - val_accuracy: 0.9780 - val_loss: 0.0851\n",
      "Epoch 10/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9840 - loss: 0.0487 - val_accuracy: 0.9786 - val_loss: 0.0874\n",
      "Training branch model 4/5\n",
      "Epoch 1/10\n",
      "375/375 - 3s - 9ms/step - accuracy: 0.8799 - loss: 0.3894 - val_accuracy: 0.9567 - val_loss: 0.1397\n",
      "Epoch 2/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9541 - loss: 0.1574 - val_accuracy: 0.9651 - val_loss: 0.1113\n",
      "Epoch 3/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9642 - loss: 0.1172 - val_accuracy: 0.9707 - val_loss: 0.0946\n",
      "Epoch 4/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9713 - loss: 0.0941 - val_accuracy: 0.9750 - val_loss: 0.0863\n",
      "Epoch 5/10\n",
      "375/375 - 2s - 6ms/step - accuracy: 0.9755 - loss: 0.0801 - val_accuracy: 0.9741 - val_loss: 0.0842\n",
      "Epoch 6/10\n",
      "375/375 - 2s - 6ms/step - accuracy: 0.9774 - loss: 0.0711 - val_accuracy: 0.9753 - val_loss: 0.0881\n",
      "Epoch 7/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9804 - loss: 0.0618 - val_accuracy: 0.9765 - val_loss: 0.0860\n",
      "Epoch 8/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9811 - loss: 0.0592 - val_accuracy: 0.9783 - val_loss: 0.0779\n",
      "Epoch 9/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9840 - loss: 0.0509 - val_accuracy: 0.9795 - val_loss: 0.0765\n",
      "Epoch 10/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9853 - loss: 0.0452 - val_accuracy: 0.9782 - val_loss: 0.0827\n",
      "Training branch model 5/5\n",
      "Epoch 1/10\n",
      "375/375 - 3s - 8ms/step - accuracy: 0.8781 - loss: 0.3998 - val_accuracy: 0.9530 - val_loss: 0.1541\n",
      "Epoch 2/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9523 - loss: 0.1583 - val_accuracy: 0.9659 - val_loss: 0.1085\n",
      "Epoch 3/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9653 - loss: 0.1151 - val_accuracy: 0.9721 - val_loss: 0.0956\n",
      "Epoch 4/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9709 - loss: 0.0925 - val_accuracy: 0.9740 - val_loss: 0.0883\n",
      "Epoch 5/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9761 - loss: 0.0782 - val_accuracy: 0.9736 - val_loss: 0.0940\n",
      "Epoch 6/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9793 - loss: 0.0671 - val_accuracy: 0.9739 - val_loss: 0.0920\n",
      "Epoch 7/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9812 - loss: 0.0603 - val_accuracy: 0.9753 - val_loss: 0.0932\n",
      "Epoch 8/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9822 - loss: 0.0562 - val_accuracy: 0.9768 - val_loss: 0.0842\n",
      "Epoch 9/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9839 - loss: 0.0501 - val_accuracy: 0.9772 - val_loss: 0.0840\n",
      "Epoch 10/10\n",
      "375/375 - 2s - 5ms/step - accuracy: 0.9844 - loss: 0.0472 - val_accuracy: 0.9787 - val_loss: 0.0878\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Ensemble accuracy on test set: 0.9830\n",
      "Variance for the first test sample: [3.1763714e-16 2.1579553e-14 2.1131971e-10 6.8279262e-12 3.5972993e-17\n",
      " 2.3079213e-15 3.4898540e-21 4.2033221e-10 9.6002892e-15 1.8025293e-11]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models, Input, Model\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_branch_model():\n",
    "    input_dim = 784  \n",
    "\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    shared_output = layers.Dense(200, activation='relu')(input_layer)\n",
    "    shared_output = layers.Dropout(0.2)(shared_output)\n",
    "\n",
    "    branch_output = layers.Dense(200, activation='relu')(shared_output)\n",
    "    branch_output = layers.Dropout(0.2)(branch_output)\n",
    "    branch_output = layers.Dense(200, activation='relu')(branch_output)\n",
    "    branch_output = layers.Dropout(0.2)(branch_output)\n",
    "    branch_output = layers.Dense(10, activation='softmax')(branch_output)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=branch_output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "num_branches = 5\n",
    "branch_models = [create_branch_model() for _ in range(num_branches)]\n",
    "\n",
    "for i, model in enumerate(branch_models):\n",
    "    print(f\"Training branch model {i + 1}/{num_branches}\")\n",
    "    model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=10, batch_size=128, verbose=2)\n",
    "\n",
    "def combine_branches(branch_models):\n",
    "    input_dim = 784  # Input dimension\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    \n",
    "    branch_outputs = [branch(input_layer) for branch in branch_models]\n",
    "    combined_output = layers.Concatenate()(branch_outputs)\n",
    "    \n",
    "    combined_model = Model(inputs=input_layer, outputs=combined_output)\n",
    "    return combined_model\n",
    "\n",
    "inference_model = combine_branches(branch_models)\n",
    "\n",
    "def ensemble_predictions(model, X, num_branches):\n",
    "    predictions = model.predict(X)\n",
    "    branch_outputs = np.split(predictions, num_branches, axis=1)\n",
    "    mean_prediction = np.mean(branch_outputs, axis=0)\n",
    "    variance_prediction = np.var(branch_outputs, axis=0)\n",
    "    return mean_prediction, variance_prediction\n",
    "\n",
    "mean_pred, var_pred = ensemble_predictions(inference_model, X_test, num_branches)\n",
    "\n",
    "predicted_classes = np.argmax(mean_pred, axis=1)\n",
    "true_classes = np.argmax(Y_test, axis=1)\n",
    "accuracy = np.mean(predicted_classes == true_classes)\n",
    "print(f\"Ensemble accuracy on test set: {accuracy:.4f}\")\n",
    "print(f\"Variance for the first test sample: {var_pred[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "900it [03:06,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Evaluation Results ==================\n",
      "Ensemble Accuracy: 0.9733\n",
      "Single Model Accuracy: 0.9767\n",
      "Average Inference Time per Sample: 0.158070 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_with_dropout(model, X):\n",
    "    return model(X, training=True)  # Ensure dropout is active during inference\n",
    "\n",
    "def evaluate_models(models_list, test_x, test_y, training=False):\n",
    "    num_samples = 900\n",
    "    sample_indices = np.random.choice(test_x.shape[0], num_samples, replace=False)\n",
    "\n",
    "    ensemble_accuracy = 0\n",
    "    single_accuracy = 0\n",
    "    inference_times = []\n",
    "\n",
    "    for i, index in tqdm(enumerate(sample_indices)):\n",
    "        img_sample = np.reshape(test_x[index], (1, -1))\n",
    "        true_label = np.reshape(test_y[index], (1, -1))\n",
    "\n",
    "        ensemble_prob = np.zeros((1, test_y.shape[1]))\n",
    "        start_time = time.time()\n",
    "\n",
    "        if training:\n",
    "            for model in models_list:\n",
    "                prob = []\n",
    "                for _ in range(5):\n",
    "                    single_prediction = predict_with_dropout(model, tf.constant(img_sample, dtype=tf.float32)).numpy()\n",
    "                    prob.append(single_prediction)\n",
    "                prob = np.array(prob)\n",
    "\n",
    "                avg_prob = prob.mean(axis=0)  # Average the probabilities across iterations\n",
    "                ensemble_prob += avg_prob\n",
    "        else:\n",
    "            for model in models_list:\n",
    "                prob = model.predict(img_sample, verbose=0)\n",
    "                ensemble_prob += prob\n",
    "        \n",
    "        end_time = time.time()\n",
    "        inference_times.append(end_time - start_time)\n",
    "\n",
    "        ensemble_prob /= len(models_list)\n",
    "        single_prob = models_list[0].predict(img_sample, verbose=0)\n",
    "\n",
    "        ensemble_accuracy += np.argmax(ensemble_prob) == np.argmax(true_label)\n",
    "        single_accuracy += np.argmax(single_prob) == np.argmax(true_label)\n",
    "\n",
    "    avg_ensemble_accuracy = ensemble_accuracy / num_samples\n",
    "    avg_single_accuracy = single_accuracy / num_samples\n",
    "    avg_inference_time = np.mean(inference_times)\n",
    "\n",
    "    print(\"================== Evaluation Results ==================\")\n",
    "    print(f\"Ensemble Accuracy: {avg_ensemble_accuracy:.4f}\")\n",
    "    print(f\"Single Model Accuracy: {avg_single_accuracy:.4f}\")\n",
    "    print(f\"Average Inference Time per Sample: {avg_inference_time:.6f} seconds\")\n",
    "    \n",
    "\n",
    "def evaluate_combined_model(combined_model, test_x, test_y, num_branches):\n",
    "    num_samples = 900\n",
    "    sample_indices = np.random.choice(test_x.shape[0], num_samples, replace=False)\n",
    "\n",
    "    ensemble_accuracy = 0\n",
    "    single_accuracy = 0\n",
    "    inference_times = []\n",
    "\n",
    "    for i, index in tqdm(enumerate(sample_indices)):\n",
    "        img_sample = np.reshape(test_x[index], (1, -1))\n",
    "        true_label = np.reshape(test_y[index], (1, -1))\n",
    "\n",
    "        start_time = time.time()\n",
    "        combined_output = combined_model.predict(img_sample, verbose=0)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        branch_outputs = np.split(combined_output, num_branches, axis=1)\n",
    "        ensemble_prob = np.mean(branch_outputs, axis=0)\n",
    "\n",
    "        single_prob = branch_outputs[0]\n",
    "        \n",
    "        inference_times.append(end_time - start_time)\n",
    "\n",
    "        ensemble_accuracy += np.argmax(ensemble_prob) == np.argmax(true_label)\n",
    "        single_accuracy += np.argmax(single_prob) == np.argmax(true_label)\n",
    "\n",
    "    avg_ensemble_accuracy = ensemble_accuracy / num_samples\n",
    "    avg_single_accuracy = single_accuracy / num_samples\n",
    "    avg_inference_time = np.mean(inference_times)\n",
    "\n",
    "    print(\"================== Evaluation Results ==================\")\n",
    "    print(f\"Ensemble Accuracy: {avg_ensemble_accuracy:.4f}\")\n",
    "    print(f\"Single Model Accuracy (First Branch): {avg_single_accuracy:.4f}\")\n",
    "    print(f\"Average Inference Time per Sample: {avg_inference_time:.6f} seconds\")\n",
    "\n",
    "\n",
    "evaluate_models(models_ensemble, X_test, Y_test)\n",
    "evaluate_models([model_mc_dropout]*5, X_test, Y_test, training=True)\n",
    "evaluate_models(models_bootstrap, X_test, Y_test)\n",
    "evaluate_combined_model(inference_model, X_test, Y_test, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "900it [03:12,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================== Ensemble Result ======================\n",
      "Accuracy: 0.0978\n",
      "Avg confidence: 0.7545\n",
      "Average Variance: 0.0053\n",
      "Average Entropy: 0.5987\n",
      "Average Inference Time per Sample: 0.164834 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_with_dropout(model, X):\n",
    "    return model(X, training=True)  # Ensure dropout is active during inference\n",
    "\n",
    "def evaluate_models_non_mnist(models_list, test_x, test_y, training=False):\n",
    "    num_samples = 900\n",
    "    sample_indices = np.random.choice(test_x.shape[0], num_samples, replace=False)\n",
    "\n",
    "    ensemble_accuracy = 0\n",
    "    single_accuracy = 0\n",
    "    inference_times = []\n",
    "\n",
    "    array_ensemble_NotMNIST = np.zeros([num_samples])\n",
    "    count_ex_n_en = 0\n",
    "    tot_prob_en = 0\n",
    "    conf = 0.2\n",
    "    \n",
    "    variance_list = []\n",
    "    entropy_list = []\n",
    "\n",
    "    for i, index in tqdm(enumerate(sample_indices)):\n",
    "        img_sample = np.reshape(test_x[index], (1, -1))\n",
    "        true_label = np.reshape(test_y[index], (1, -1))\n",
    "\n",
    "        ensemble_prob = np.zeros((1, test_y.shape[1]))\n",
    "        individual_probs = []\n",
    "        start_time = time.time()\n",
    "\n",
    "        \n",
    "        if training:\n",
    "            for model in models_list:\n",
    "                # Run the model n_iter times with dropout enabled\n",
    "                prob = np.array([predict_with_dropout(model, tf.constant(img_sample, dtype=tf.float32)).numpy() for _ in range(5)])\n",
    "                avg_prob = prob.mean(axis=0) \n",
    "                ensemble_prob += avg_prob\n",
    "                individual_probs.append(avg_prob)\n",
    "        else:\n",
    "            for model in models_list:\n",
    "                prob = model.predict(img_sample, verbose=0)\n",
    "                ensemble_prob += prob\n",
    "                individual_probs.append(prob)\n",
    "\n",
    "        end_time = time.time()\n",
    "        inference_times.append(end_time - start_time)\n",
    "\n",
    "        ensemble_prob /= len(models_list)\n",
    "        single_prob = models_list[0].predict(img_sample, verbose=0)\n",
    "        ensemble_accuracy += np.argmax(ensemble_prob) == np.argmax(true_label)\n",
    "        single_accuracy += np.argmax(single_prob) == np.argmax(true_label)\n",
    "        \n",
    "        individual_probs = np.array(individual_probs).squeeze()\n",
    "        variance = np.var(individual_probs, axis=0).mean()\n",
    "        variance_list.append(variance)\n",
    "        \n",
    "        pred_entropy = entropy(ensemble_prob.squeeze())\n",
    "        entropy_list.append(pred_entropy)\n",
    "\n",
    "        idx_sample = np.argmax(ensemble_prob)\n",
    "        max_prob = ensemble_prob[0, idx_sample]\n",
    "        array_ensemble_NotMNIST[i] = max_prob\n",
    "        tot_prob_en += max_prob\n",
    "\n",
    "        if max_prob >= conf:\n",
    "            count_ex_n_en += 1\n",
    "        # print(f\"{i + 1}th sample: label = {idx_sample}, Confidence = {max_prob}\")\n",
    "\n",
    "    avg_ensemble_accuracy = ensemble_accuracy / num_samples\n",
    "    avg_single_accuracy = single_accuracy / num_samples\n",
    "    avg_inference_time = np.mean(inference_times)\n",
    "    avg_variance = np.mean(variance_list)\n",
    "    avg_entropy = np.mean(entropy_list)\n",
    "\n",
    "    print(\"====================== Ensemble Result ======================\")\n",
    "    print(f\"Accuracy: {avg_ensemble_accuracy:.4f}\")\n",
    "    print(f\"Avg confidence: {tot_prob_en / num_samples:.4f}\")\n",
    "    print(f\"Satisfying Ensemble NotMNIST: {count_ex_n_en}\")\n",
    "    print(f\"Average Variance: {avg_variance:.4f}\")\n",
    "    print(f\"Average Entropy: {avg_entropy:.4f}\")\n",
    "    print(f\"Average Inference Time per Sample: {avg_inference_time:.6f} seconds\")\n",
    "\n",
    "\n",
    "def evaluate_combined_model_non_mnist(combined_model, test_x, test_y, num_branches):\n",
    "    import time\n",
    "    from scipy.stats import entropy\n",
    "\n",
    "    num_samples = 900\n",
    "    sample_indices = np.random.choice(test_x.shape[0], num_samples, replace=False)\n",
    "\n",
    "    ensemble_accuracy = 0\n",
    "    single_accuracy = 0\n",
    "    inference_times = []\n",
    "\n",
    "    array_ensemble_NotMNIST = np.zeros([num_samples])\n",
    "    count_ex_n_en = 0\n",
    "    tot_prob_en = 0\n",
    "    conf = 0.2\n",
    "\n",
    "    variance_list = []\n",
    "    entropy_list = []\n",
    "\n",
    "    for i, index in tqdm(enumerate(sample_indices)):\n",
    "        img_sample = np.reshape(test_x[index], (1, -1))\n",
    "        true_label = np.reshape(test_y[index], (1, -1))\n",
    "\n",
    "        start_time = time.time()\n",
    "        combined_output = combined_model.predict(img_sample, verbose=0)\n",
    "        \n",
    "        branch_outputs = np.split(combined_output, num_branches, axis=1)\n",
    "        ensemble_prob = np.mean(branch_outputs, axis=0)\n",
    "\n",
    "        single_prob = branch_outputs[0]\n",
    "\n",
    "        end_time = time.time()\n",
    "        inference_times.append(end_time - start_time)\n",
    "\n",
    "        ensemble_accuracy += np.argmax(ensemble_prob) == np.argmax(true_label)\n",
    "        single_accuracy += np.argmax(single_prob) == np.argmax(true_label)\n",
    "\n",
    "        branch_predictions = np.array(branch_outputs).squeeze()\n",
    "        variance = np.var(branch_predictions, axis=0).mean()\n",
    "        variance_list.append(variance)\n",
    "\n",
    "        pred_entropy = entropy(ensemble_prob.squeeze())\n",
    "        entropy_list.append(pred_entropy)\n",
    "\n",
    "        idx_sample = np.argmax(ensemble_prob)\n",
    "        max_prob = ensemble_prob[0, idx_sample]\n",
    "        array_ensemble_NotMNIST[i] = max_prob\n",
    "        tot_prob_en += max_prob\n",
    "\n",
    "        if max_prob >= conf:\n",
    "            count_ex_n_en += 1\n",
    "\n",
    "    avg_ensemble_accuracy = ensemble_accuracy / num_samples\n",
    "    avg_single_accuracy = single_accuracy / num_samples\n",
    "    avg_inference_time = np.mean(inference_times)\n",
    "    avg_variance = np.mean(variance_list)\n",
    "    avg_entropy = np.mean(entropy_list)\n",
    "\n",
    "    print(\"====================== Ensemble Result ======================\")\n",
    "    print(f\"Accuracy: {avg_ensemble_accuracy:.4f}\")\n",
    "    print(f\"Avg confidence: {tot_prob_en / num_samples:.4f}\")\n",
    "    print(f\"Satisfying Ensemble NotMNIST: {count_ex_n_en}\")\n",
    "    print(f\"Average Variance: {avg_variance:.4f}\")\n",
    "    print(f\"Average Entropy: {avg_entropy:.4f}\")\n",
    "    print(f\"Average Inference Time per Sample: {avg_inference_time:.6f} seconds\")\n",
    "    \n",
    "    \n",
    "evaluate_models_non_mnist(models_ensemble, NotMNIST_x, NotMNIST_y)\n",
    "evaluate_models_non_mnist([model_mc_dropout]*5, NotMNIST_x, NotMNIST_y, training=True)\n",
    "evaluate_models_non_mnist(models_bootstrap, NotMNIST_x, NotMNIST_y)\n",
    "evaluate_combined_model_non_mnist(inference_model, NotMNIST_x, NotMNIST_y, 5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
